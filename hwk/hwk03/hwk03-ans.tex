\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



% \titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Homework 3}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5204: Statistical Inference}\\
    {\normalsize Columbia University}
}
\date{\normalsize December 14, 2020}

\begin{document}

\maketitle

%' ============================================================================================================================================================
\section{Question 1} \noindent
Suppose that \(\bm{X} \overset{\mathrm{iid}}{\sim} \chi^2(\theta)\), where \(\theta \in \mathbb{N}\) is unknown. We would like to test 
\(H_0 : \theta \le 8\) against \(H_A : \theta > 8\), using a UMP test \(\delta^*\) with a specified significance \(\alpha_* \in (0,1)\). 
The joint density of \(\bm{X}\) is given by 
\begin{align*}
    f(\mathbf{x} \,|\, \theta)
    = \prod_{i=1}^n \frac{x_i^{\theta / 2 - 1} \mathrm{e}^{-x/2}}{2^{\theta/2} \Gamma(\theta/2)}
    % = \frac{1}{2^{n\theta/2}\Gamma^n(\theta/2)} \left( \prod_{i=1}^n x_i \right)^{n(\theta/2 - 1)} \cdot \mathrm{exp}\left( -\frac{1}{2} \sum_{i=1}^n x_i \right)
    = 2^{-n\theta/2} \cdot \Gamma^{-n}(\theta/2) \cdot \left( \prod_{i=1}^n x_i \right)^{n(\theta/2 - 1)} \cdot \mathrm{exp}\left( -\frac{1}{2} \sum_{i=1}^n x_i \right)
\end{align*}
To determine \(\delta^*\), we will look at the likelihood ratio. If we have two values \(\theta_1,\theta_2\) such that \(\theta_1 < \theta_2\), then then 
likelihood ratio is 
\begin{align*}
    \frac{f(\mathbf{x} \,|\, \theta_2)}{f(\mathbf{x} \,|\, \theta_1)}
    &= \dfrac{
        2^{-n\theta_1/2} \cdot \Gamma^{-n}(\theta_2/2) \cdot \left( \prod_i x_i \right)^{n(\theta_2/2 - 1)} \cdot \mathrm{exp}\left( -\frac{1}{2} \sum_i x_i \right)
    }{
        2^{-n\theta_1/2} \cdot \Gamma^{-n}(\theta_1/2) \cdot \left( \prod_i x_i \right)^{n(\theta_1/2 - 1)} \cdot \mathrm{exp}\left( -\frac{1}{2} \sum_i x_i \right)
    } \\
    &= 2^{n(\theta_1 - \theta_2)/2} \left( \frac{\Gamma(\theta_1/2)}{\Gamma(\theta_2/2)} \right)^n \left( \prod_{i=1}^n x_i \right)^{n(\theta_2 - \theta_1)/2},
\end{align*}
which is a monotone increasing function of the test statistic \(\prod_{i=1}^n x_i\). Therefore, the UMP test is \(\delta^* :\) reject \(H_0\) if 
\(\prod_{i=1}^n X_i \ge c_*\), where \(c_*\) is chosen such that the test has a significance level \(\alpha_*\). Taking the log of both sides gives us 
\(\sum_{i=1}^8 \log X_i \ge \log c_* \coloneqq k\). 

%' ============================================================================================================================================================
\section{Question 2} \noindent


%' ============================================================================================================================================================
\section{Question 8} \noindent
By definition, the least-squares estimate for the intercept is \(\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}\).
Rearranging gives us \(\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}\), and so the least-squares line \(y = \hat{\beta}_0 + \hat{\beta}_1 x\) will always 
pass through the point \((\bar{x}, \bar{y})\).

%' ============================================================================================================================================================
\section{Question 9} \noindent
\begin{itemize}
    \item[(a)] The least-squares coefficients for the model are given by \(\hat{\beta}_0 = 40.9\) and \(\hat{\beta}_1 = 0.548\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 10} \noindent
Let \((\mathbf{x}, \mathbf{y})\) be the vectors of observations for the predictor \(x\) and the response \(Y\). 
From the data, we have \(\bar{x} = 2.33\) and \(\bar{y} = 0.81\).
Here we are assuming that \(Y = \beta_0 + \beta_1 x + \epsilon\), where \(\epsilon \sim \mathrm{N}(0, \sigma^2)\).
\begin{itemize}
    \item[(a)] The MLEs for \(\beta_0\), \(\beta_1\), and \(\sigma^2\) are given by 
    \begin{align*}
        % \hat{\beta}_{1,\mathrm{MLE}}
        \hat{\beta}_{1}
        = \frac{(\mathbf{y} - \bar{y}\mathbf{1})^T(\mathbf{x} - \bar{x}\mathbf{1})}{\| \mathbf{x} - \bar{x}\mathbf{1} \|^2}
        = 0.685
        % ~~~
        ~,~
        % \hat{\beta}_{0,\mathrm{MLE}}
        \hat{\beta}_0
        = \bar{y} - \hat{\beta}_1 \bar{x}
        = -0.786
        % ~~~\mathrm{and}~~~
        ~,~
        % \hat{\sigma}^2_{\mathrm{MLE}}
        \hat{\sigma}^2
        = \frac{\| \mathbf{y} - \hat{\beta}_0 \mathbf{1} - \hat{\beta}_1 \mathbf{x} \|^2}{n}
        = 0.938.
    \end{align*}
    \item[(b)] The variance of \(\hat{\beta}_0\) and \(\hat{\beta}_1\) is given by 
    \begin{align*}
        \mathrm{Var}\big[ \hat{\beta}_1 \big]
        = \frac{\sigma^2}{\| \mathbf{x} - \bar{x}\mathbf{1} \|^2}
        = 0.0277 \sigma^2
        % ~~~\mathrm{and}~~~
        ~,~
        \mathrm{Var}\big[ \hat{\beta}_0 \big]
        = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\| \mathbf{x} - \bar{x}\mathbf{1} \|^2} \right)
        = 0.25 \sigma^2.
    \end{align*}
    \item[(c)] The covariance between \(\hat{\beta}_0\) and \(\hat{\beta}_1\), and therefore the correltaion, is 
    \begin{align*}
        \mathrm{Cov}\big[ \hat{\beta}_0, \hat{\beta}_1 \big]
        = - \frac{\bar{x} \sigma^2}{\| \mathbf{x} - \bar{x}\mathbf{1} \|^2}
        = -0.0646 \sigma^2
        % ~~~\mathrm{and}~~~
        ~,~
        \mathrm{Cor}\big[ \hat{\beta}_0, \hat{\beta}_1 \big] 
        % = \frac{\mathrm{Cov}\big[ \hat{\beta}_0, \hat{\beta}_1 \big]}{\sqrt{\mathrm{Var}\big[ \hat{\beta}_0 \big]} \cdot \sqrt{\mathrm{Var}\big[ \hat{\beta}_1 \big]}}
        = \frac{\mathrm{Cov}\big[ \hat{\beta}_0, \hat{\beta}_1 \big]}{\sqrt{\mathrm{Var}\big[ \hat{\beta}_0 \big] \cdot \mathrm{Var}\big[ \hat{\beta}_1 \big]}}
        % = \frac{-1.5 \sigma^2}{\sqrt{0.0277 \sigma^2} \cdot \sqrt{0.25 \sigma^2}}
        = -0.775.
    \end{align*}
\end{itemize}

%' ============================================================================================================================================================
\section{Question 11} \noindent
Suppose \(\beta_0, \beta_1\) are the coefficients from the linear model in question 10, and we want to estimate \(\theta = 3 \beta_0 - 2 \beta_1 + 5\).
Because \(\hat{\beta}_0\) and \(\hat{\beta}_1\) are unbiased estimators for the coefficients, we can estimate \(\theta\)
with \(\hat{\theta} = 3 \hat{\beta}_0 - 2 \hat{\beta}_1 + 5\). The MSE of \(\hat{\theta}\), which is just its variance, is given by 
\begin{align*}
    \mathbb{E}\Big[ (\hat{\theta} - \theta)^2 \Big]
    = \mathrm{Var} \big[ 3 \hat{\beta}_0 - 2 \hat{\beta}_1 + 5 ] 
    = 9 \mathrm{Var}\big[ \hat{\beta}_0 \big] + 4 \mathrm{Var}\big[ \hat{\beta}_1 \big] - 6 \mathrm{Cov}\big[ \hat{\beta}_0, \hat{\beta}_1 \big]
    = 10.549 \sigma^2.
\end{align*}

%' ============================================================================================================================================================
\section{Question 12} \noindent
We know that the MLE and least-squares estimates of \(\beta_0\) and \(\beta_1\) are the same, so \(\hat{\beta}_0 = -0.786\) and \(\hat{\beta}_1 = 0.685\).
Using this linear model, when \(x = 2\), we predict \(\hat{Y} = -0.786 _ 0.685 \cdot 2 = 0.584\). The variance of \(\hat{Y}\) is given by 
\begin{align*}
    \mathbb{E} \Big[ (\hat{Y} - Y)^2 \Big]
    = \sigma^2 \left( 1 + \frac{1}{n} + \frac{(2 - \bar{x})^2}{\| \mathbf{x} - \bar{x}\mathbf{1} \|^2} \right)
    = 1.103 \sigma^2.
\end{align*}

\end{document}