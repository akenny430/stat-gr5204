\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



% \titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Homework 1}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5204: Statistical Inference}\\
    {\normalsize Columbia University}
}
\date{\normalsize November 10, 2020}

\begin{document}

\maketitle

%' ============================================================================================================================================================
\section{Question 1} \noindent
When rolling two dice, there are six possible ways for their total to sum up to seven: \((1,6)\), \((2,5)\), \((3,4)\), \((4,3)\), \((5,2)\), and \((6,1)\), 
so the probability of the sum being seven is \(6 / 36 = 1 / 6\). If \(X\) is the number of trials where the total of both rolls is seven, then we can think 
of \(X \sim \mathrm{Bin}(120, 1/6)\), and so \(\mathbb{E}X = 20\) and \(\mathrm{Var}X = 50 / 3\). 
% Then the random variable 
% \(Z = \frac{X - 20}{\sqrt{5/36}} \sim \mathrm{N}(0, 1)\)
% Note then that \(\sqrt{\mathrm{Var}X / n} = \sqrt{5}/6\).
Using the Central Limit Theorem, we then have 
\begin{align*}
    \mathrm{Pr}\big( |X - 20| \le k \big)
    = \mathrm{Pr} \left( \left| \frac{X - 20}{\sqrt{50/3}} \right| \le k \sqrt{\frac{3}{50}} \right)
    = 2 \Phi \left( k \sqrt{\frac{3}{50}} \right) - 1
    \overset{\text{set}}{=} 0.95
    ~~\Longrightarrow~~ \Phi \left( k \sqrt{\frac{3}{50}} \right) = 0.975.
\end{align*}
Using a table of values for \(\Phi(z)\), we can see that \(k \sqrt{3 / 50} = 1.96\), and so \(k = 1.96 \sqrt{50/3} \approx 8\).

%' ============================================================================================================================================================
\section{Question 2} \noindent
Let \(X \sim \mathrm{Pois}(10)\), and so \(\mathbb{E}X = \mathrm{Var}X = 10\). Using the CLT without any continuity correction, we have 
\((X - 10)/\sqrt{10} \approx \mathrm{N}(0,1)\), and so 
\begin{align*}
    \mathrm{Pr}(8 \le X \le 12)
    % = \mathrm{Pr} \left( \frac{8 - 10}{\sqrt{10}} \le \frac{X - 10}{\sqrt{10}} \le \frac{12 - 10}{\sqrt{10}} \right)
    = \mathrm{Pr} \left( \frac{8 - 10}{\sqrt{10}} \le Z \le \frac{12 - 10}{\sqrt{10}} \right)
    % = \mathrm{Pr} \big(- \sqrt{2 / 5} \le Z \le \sqrt{2 / 5} \big) \\
    = \mathrm{Pr} \big( |Z| \le \sqrt{2/5} \big) 
    % &\approx \Phi \big( \sqrt{2/5} \big) - \Phi \big( -\sqrt{2/5} \big)
    \approx 2 \Phi \big( \sqrt{2/5} \big) - 1 
    = 0.4714.
\end{align*}
If we do use continuity correction, then we have 
\begin{align*}
    \mathrm{Pr}(8 \le X \le 12)
    &\approx \mathrm{Pr}(7.5 \le X \le 12.5) \\
    &= \mathrm{Pr} \left( \frac{7.5 - 10}{\sqrt{10}} \le Z \le \frac{12.5 - 10}{\sqrt{10}} \right)
    = \mathrm{Pr} \big( |Z| \le 2.5/\sqrt{10} \big) 
    \approx 2 \Phi \big( 2.5/\sqrt{10} \big) - 1 
    = 0.5704.
\end{align*}
% The exact probability can be found as 
% \begin{align*}
%     \mathrm{Pr}(8 \le X \le 12)
%     = \sum_{x = 8}^{12} \frac{10^x \mathrm{e}^{-10}}{x!}
%     = 
% \end{align*}
% so the continuity correction significantly improves the accuracy of our estimation.

%' ============================================================================================================================================================
\section{Question 3} \noindent
We are assuming that when a program is run, an execution error will occur with probability \(\theta \in [0,1]\). If \(X\) is whether or not an execution error
occurs, we have \(X \sim \mathrm{Ber}(\theta)\), and \(f(x \,|\, \theta) = \theta^x (1 - \theta)^{1 - x}\) for \(x = \{0,1\}\). We also believe that 
\(\theta \sim \mathrm{Unif}(0,1)\), and so \(\xi(\theta) = 1\) for \(0 \le \theta \le 1\).
\begin{itemize}
    \item[(a)] After 25 runs of the program we have 10 erros, so \(f(\mathbf{x} \,|\, \theta) = \theta^{10}(1 - \theta)^{15}\). The marginal distribution of 
    \(\mathbf{x}\) is given by 
    \begin{align*}
        g_{\bm{X}}(\mathbf{x})
        = \int_{\Theta} f(\mathbf{x} \,|\, \theta) \cdot \xi(\theta) \;\mathrm{d}\theta 
        = \int_0^1 \theta^{10} (1 - \theta)^{15} \cdot 1 \;\mathrm{d}\theta
        = \int_0^1 \theta^{11 - 1} (1 - \theta)^{16 - 1} \;\mathrm{d}\theta
        = \mathrm{B}(11, 16),
    \end{align*}
    and so the posterior pdf of \(\theta\) is 
    \begin{align*}
        \xi(\theta \,|\, \mathbf{x})
        = \frac{f(\mathbf{x} \,|\, \theta) \cdot \xi(\theta)}{g_{\bm{X}}(\mathbf{x})}
        = \frac{\theta^{10} (1 - \theta)^{15} \cdot 1}{\mathrm{B}(11, 16)}
        = \frac{\theta^{11 - 1} (1 - \theta)^{16 - 1}}{\mathrm{B}(11, 16)}.
    \end{align*}
    That is, \(\theta \sim \mathrm{Beta}(11, 16)\). 
    \item[(b)] If we are using squared error loss, then our Bayes' estimate is 
    \(\delta^*(\mathbf{x}) = \mathbb{E}(\theta \,|\, \mathbf{x}) = 11/27\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 4} \noindent
We believe that \(\theta \sim \mathrm{Beta}(3, 4)\), where \(\theta \in [0,1]\) is the proportion of bad apples in the lot. 
% For each pick, if \(X \in \{0,1\}\) denotes whether or not a bad apple was chosen, we have \(f(x \,|\, \theta) = \theta^x (1 - \theta)^{1 - x}\)
% After randomly choosing 10 applies, we find that three of them are bad, so our posterior 
% Since choosing apples and determining whether or not it is bad is sampling from a Bernoulli distribution
Choosing apples from the lot is essentially sampling from a Bernoulli distribution with parameter \(\theta\),
and we know that Beta distributions are closed under sampling from a Bernoulli distribution. After choosing 
10 apples, we find that three of them are bad, so our posterior distribution becomes \(\theta \,|\, \mathbf{x} \sim \mathrm{Beta}(3+3, 4+7) = \mathrm{Beta}(6,11)\).
If we use squared error loss, our Bayes' estimate is then \(\delta^*(\mathbf{x}) = \mathbb{E}(\theta \,|\, \mathbf{x}) = 6 / 17\).

%' ============================================================================================================================================================
\section{Question 5} \noindent
Let \(\bm{X} = (X_1, \ldots, X_n)^T\) be a random sample from \(X \sim \mathrm{Unif}(\theta, 2\theta)\), where \(\theta > 0\). The likelihood function is then given by 
\(f(\mathbf{x} \,|\, \theta) = 1 / \theta^n\) when \(\theta \le x_i \le 2\theta\) for \(i \in \{1,\ldots,n\}\). We can re-frame the boundaries of the likelihood 
function using order statistics. Since we need every observation \(x_i \in [\theta, 2\theta]\), it follows that 
\(\theta \le x_{(1)} \le \cdots \le x_{(n)} \le 2\theta\), where \(x_{(j)}\) is the \(j\)th order statistics; namely, we have \(\theta \le x_{(1)}\) 
and \(x_{(n)} \le 2 \theta\). From the second inequality,
we have \(x_{(n)}/2 \le \theta\), and so the possible values of \(\theta\) are \(x_{(n)}/2 \le \theta \le x_{(1)}\). In other words, 
even though we had the original parameter space \(\Theta = (0, \infty)\), because the bounds of the density functions depended on \(\theta\), we were able to 
restrict \(\theta\) to a new parameter space \(\tilde{\Theta} = [x_{(n)}/2, x_{(1)}]\). We can see that our likelihood function is monotone decreasing, and so 
it will be maximized by the smallest possible value of \(\theta\). Therefore, the MLE of \(\theta\) is \(\hat{\theta}(\bm{X}) = X_{(n)} / 2\).

%' ============================================================================================================================================================
\section{Question 6} \noindent
Suppose that \(\bm{X} = (X_1, X_2, X_3)^T\) are each exponentially distributed with \(\mathbb{E}X_i = i\theta\), where \(\theta > 0\). This implies that 
\(X_i \sim \mathrm{Exp}(1 / i\theta)\), and so \(f(x_i \,|\, \theta) = \mathrm{e}^{-x_i / i\theta} / i\theta\). 
\begin{itemize}
    \item[(a)] The likelihood function is given by 
    \begin{align*}
        f(\mathbf{x} \,|\, \theta)
        = \prod_{i=1}^3 f(x_i \,|\, \theta)
        = \prod_{i=1}^3 \frac{\mathrm{e}^{-x_i / i\theta}}{i \theta}
        = \frac{1}{6 \theta^3} \mathrm{exp} \left( - \frac{1}{\theta} \sum_{i=1}^3 \frac{x_i}{i} \right),
        % = \frac{1}{6 \theta^3} \mathrm{exp} \left( - \frac{1}{\theta} \sum_{i=1}^3 \frac{x_i}{i} \right).
    \end{align*}
    % For notational ease, let \(\varphi(\mathbf{x}) = \sum_{i=1}^3 x_i / i\), and so \(f(\mathbf{x} \,|\, \theta) = \mathrm{e}^{- \varphi(\mathbf{x}) / \theta} / 6 \theta^3\)
    and the corresponding log-likelihood function is given by 
    \(\ell(\mathbf{x} \,|\, \theta) = -3 \log (6 \theta) - \frac{1}{\theta} \sum_{i=1}^3 x_i / i.\)
    % and the corresponding log-likelihood function is given by 
    % \begin{align*}
    %     \ell(\mathbf{x} \,|\, \theta)
    %     = \log f(\mathbf{x} \,|\, \theta)
    %     = -3 \log (6 \theta) - \frac{1}{\theta} \sum_{i=1}^3 \frac{x_i}{i}.
    % \end{align*}
    Differentiating \(\ell(\mathbf{x} \,|\, \theta)\) with respect to \(\theta\), setting to 0, and solving for \(\theta\) gives us the MLE:
    \begin{align*}
        \frac{\partial \ell}{\partial \theta}  
        = - \frac{3}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^3 \frac{x_i}{i} 
        \overset{\text{set}}{=} 0
        ~~\Longrightarrow~~
        \hat{\theta}(\bm{X}) = \frac{1}{3} \sum_{i=1}^3 \frac{X_i}{i}
    \end{align*}
    \item[(b)] Let \(\psi = 1 / \theta\), and we believe that \(\psi \sim \mathrm{Gamma}(\alpha, \beta)\), i.e. 
    \(\xi(\psi) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \psi^{\alpha} \mathrm{e}^{-\beta \psi}\) for \(\psi > 0\). 
    For notational ease, let \(\varphi(\mathbf{x}) = \sum_{i=1}^3 x_i / i\); the likelihood function of \(\psi\) is then given by 
    \(f(\mathbf{x} \,|\, \psi) = \psi^3 \mathrm{e}^{- \varphi(\mathbf{x}) \psi} / 6\). We have 
    \begin{align*}
        \xi(\psi \,|\, \mathbf{x})
        \propto f(\mathbf{x} \,|\, \psi) \cdot \xi(\psi) 
        = \frac{1}{6} \psi^3 \mathrm{e}^{- \varphi(\mathbf{x}) \psi} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \psi^{\alpha - 1} \mathrm{e}^{-\beta \psi}
        \propto \psi^{\alpha + 3 - 1} \mathrm{e}^{-(\beta + \varphi(\mathbf{x})) \psi}.
    \end{align*}
    This is very similar to a Gamma distribution with parameters \(\tilde{\alpha} = \alpha + 3\) and \(\tilde{\beta} = \beta + \varphi(\mathbf{x})\), and adding 
    in the normalizing constants would make it so. Therefore, we conclude that \(\psi \,|\, \mathbf{x} \sim \mathrm{Gamma} \big( \alpha + 3, \beta + \varphi(\mathbf{x}) \big)\).
    
\end{itemize}


\end{document}