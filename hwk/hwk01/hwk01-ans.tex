\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



% \titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Homework 1}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5204: Statistical Inference}\\
    {\normalsize Columbia University}
}
\date{\normalsize November 10, 2020}

\begin{document}

\maketitle

%' ============================================================================================================================================================
\section{Question 1} \noindent
When rolling two dice, there are six possible ways for their total to sum up to seven: \((1,6)\), \((2,5)\), \((3,4)\), \((4,3)\), \((5,2)\), and \((6,1)\), 
so the probability of the sum being seven is \(6 / 36 = 1 / 6\). If \(X\) is the number of trials where the total of both rolls is seven, then we can think 
of \(X \sim \mathrm{Bin}(120, 1/6)\), and so \(\mathbb{E}X = 20\) and \(\mathrm{Var}X = 50 / 3\). 
% Then the random variable 
% \(Z = \frac{X - 20}{\sqrt{5/36}} \sim \mathrm{N}(0, 1)\)
% Note then that \(\sqrt{\mathrm{Var}X / n} = \sqrt{5}/6\).
Using the Central Limit Theorem, we then have 
\begin{align*}
    \mathrm{Pr}\big( |X - 20| \le k \big)
    = \mathrm{Pr} \left( \left| \frac{X - 20}{\sqrt{50/3}} \right| \le k \sqrt{\frac{3}{50}} \right)
    = 2 \Phi \left( k \sqrt{\frac{3}{50}} \right) - 1
    \overset{\text{set}}{=} 0.95
    ~~\Longrightarrow~~ \Phi \left( k \sqrt{\frac{3}{50}} \right) = 0.975.
\end{align*}
Using a table of values for \(\Phi(z)\), we can see that \(k \sqrt{3 / 50} = 1.96\), and so \(k = 1.96 \sqrt{50/3} \approx 8\).

%' ============================================================================================================================================================
\section{Question 2} \noindent
Let \(X \sim \mathrm{Pois}(10)\), and so \(\mathbb{E}X = \mathrm{Var}X = 10\). Using the CLT without any continuity correction, we have 
\((X - 10)/\sqrt{10} \approx \mathrm{N}(0,1)\), and so 
\begin{align*}
    \mathrm{Pr}(8 \le X \le 12)
    % = \mathrm{Pr} \left( \frac{8 - 10}{\sqrt{10}} \le \frac{X - 10}{\sqrt{10}} \le \frac{12 - 10}{\sqrt{10}} \right)
    = \mathrm{Pr} \left( \frac{8 - 10}{\sqrt{10}} \le Z \le \frac{12 - 10}{\sqrt{10}} \right)
    % = \mathrm{Pr} \big(- \sqrt{2 / 5} \le Z \le \sqrt{2 / 5} \big) \\
    = \mathrm{Pr} \big( |Z| \le \sqrt{2/5} \big) 
    % &\approx \Phi \big( \sqrt{2/5} \big) - \Phi \big( -\sqrt{2/5} \big)
    \approx 2 \Phi \big( \sqrt{2/5} \big) - 1 
    = 0.4714.
\end{align*}
If we do use continuity correction, then we have 
\begin{align*}
    \mathrm{Pr}(8 \le X \le 12)
    &\approx \mathrm{Pr}(7.5 \le X \le 12.5) \\
    &= \mathrm{Pr} \left( \frac{7.5 - 10}{\sqrt{10}} \le Z \le \frac{12.5 - 10}{\sqrt{10}} \right)
    = \mathrm{Pr} \big( |Z| \le 2.5/\sqrt{10} \big) 
    \approx 2 \Phi \big( 2.5/\sqrt{10} \big) - 1 
    = 0.5704.
\end{align*}
% The exact probability can be found as 
% \begin{align*}
%     \mathrm{Pr}(8 \le X \le 12)
%     = \sum_{x = 8}^{12} \frac{10^x \mathrm{e}^{-10}}{x!}
%     = 
% \end{align*}
% so the continuity correction significantly improves the accuracy of our estimation.

%' ============================================================================================================================================================
\section{Question 3} \noindent
We are assuming that when a program is run, an execution error will occur with probability \(\theta \in [0,1]\). If \(X\) is whether or not an execution error
occurs, we have \(X \sim \mathrm{Ber}(\theta)\), and \(f(x \,|\, \theta) = \theta^x (1 - \theta)^{1 - x}\) for \(x = \{0,1\}\). We also believe that 
\(\theta \sim \mathrm{Unif}(0,1)\), and so \(\xi(\theta) = 1\) for \(0 \le \theta \le 1\).
\begin{itemize}
    \item[(a)] After 25 runs of the program we have 10 erros, so \(f(\mathbf{x} \,|\, \theta) = \theta^{10}(1 - \theta)^{15}\). The marginal distribution of 
    \(\bm{X}\) is given by 
    \begin{align*}
        g_{\bm{X}}(\mathbf{x})
        = \int_{\Theta} f(\mathbf{x} \,|\, \theta) \cdot \xi(\theta) \;\mathrm{d}\theta 
        = \int_0^1 \theta^{10} (1 - \theta)^{15} \cdot 1 \;\mathrm{d}\theta
        = \int_0^1 \theta^{11 - 1} (1 - \theta)^{16 - 1} \;\mathrm{d}\theta
        = \mathrm{B}(11, 16),
    \end{align*}
    and so the posterior pdf of \(\theta\) is 
    \begin{align*}
        \xi(\theta \,|\, \mathbf{x})
        = \frac{f(\mathbf{x} \,|\, \theta) \cdot \xi(\theta)}{g_{\bm{X}}(\mathbf{x})}
        = \frac{\theta^{10} (1 - \theta)^{15} \cdot 1}{\mathrm{B}(11, 16)}
        = \frac{\theta^{11 - 1} (1 - \theta)^{16 - 1}}{\mathrm{B}(11, 16)}.
    \end{align*}
    That is, \(\theta \,|\, \mathbf{x} \sim \mathrm{Beta}(11, 16)\). 
    \item[(b)] If we are using squared error loss, then our Bayes' estimate is 
    \(\delta^*(\mathbf{x}) = \mathbb{E}(\theta \,|\, \mathbf{x}) = 11/27\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 4} \noindent
We believe that \(\theta \sim \mathrm{Beta}(3, 4)\), where \(\theta \in [0,1]\) is the proportion of bad apples in the lot. 
% For each pick, if \(X \in \{0,1\}\) denotes whether or not a bad apple was chosen, we have \(f(x \,|\, \theta) = \theta^x (1 - \theta)^{1 - x}\)
% After randomly choosing 10 applies, we find that three of them are bad, so our posterior 
% Since choosing apples and determining whether or not it is bad is sampling from a Bernoulli distribution
Choosing apples from the lot is essentially sampling from a Bernoulli distribution with parameter \(\theta\),
and we know that Beta distributions are closed under sampling from a Bernoulli distribution. After choosing 
10 apples, we find that three of them are bad, so our posterior distribution becomes \(\theta \,|\, \mathbf{x} \sim \mathrm{Beta}(3+3, 4+7) = \mathrm{Beta}(6,11)\).
If we use squared error loss, our Bayes' estimate is then \(\delta^*(\mathbf{x}) = \mathbb{E}(\theta \,|\, \mathbf{x}) = 6 / 17\).

%' ============================================================================================================================================================
\section{Question 5} \noindent
Let \(\bm{X} = (X_1, \ldots, X_n)^T\) be a random sample from \(X \sim \mathrm{Unif}(\theta, 2\theta)\), where \(\theta > 0\). The likelihood function is then given by 
\(f(\mathbf{x} \,|\, \theta) = 1 / \theta^n\) when \(\theta \le x_i \le 2\theta\) for \(i \in \{1,\ldots,n\}\). We can re-frame the boundaries of the likelihood 
function using order statistics. Since we need every observation \(x_i \in [\theta, 2\theta]\), it follows that 
\(\theta \le x_{(1)} \le \cdots \le x_{(n)} \le 2\theta\), where \(x_{(j)}\) is the \(j\)th order statistics; namely, we have \(\theta \le x_{(1)}\) 
and \(x_{(n)} \le 2 \theta\). From the second inequality,
we have \(x_{(n)}/2 \le \theta\), and so the possible values of \(\theta\) are \(x_{(n)}/2 \le \theta \le x_{(1)}\). In other words, 
even though we had the original parameter space \(\Theta = (0, \infty)\), because the bounds of the density functions depended on \(\theta\), we were able to 
restrict \(\theta\) to a new parameter space \(\tilde{\Theta} = [x_{(n)}/2, x_{(1)}]\). We can see that our likelihood function is monotone decreasing, and so 
it will be maximized by the smallest possible value of \(\theta\). Therefore, the MLE of \(\theta\) is \(\hat{\theta}(\bm{X}) = X_{(n)} / 2\).

%' ============================================================================================================================================================
\section{Question 6} \noindent
Suppose that \(\bm{X} = (X_1, X_2, X_3)^T\) are each exponentially distributed with \(\mathbb{E}X_i = i\theta\), where \(\theta > 0\). This implies that 
\(X_i \sim \mathrm{Exp}(1 / i\theta)\), and so \(f(x_i \,|\, \theta) = \mathrm{e}^{-x_i / i\theta} / i\theta\). 
\begin{itemize}
    \item[(a)] The likelihood function is given by 
    \begin{align*}
        f(\mathbf{x} \,|\, \theta)
        = \prod_{i=1}^3 f(x_i \,|\, \theta)
        = \prod_{i=1}^3 \frac{\mathrm{e}^{-x_i / i\theta}}{i \theta}
        = \frac{1}{6 \theta^3} \mathrm{exp} \left( - \frac{1}{\theta} \sum_{i=1}^3 \frac{x_i}{i} \right),
        % = \frac{1}{6 \theta^3} \mathrm{exp} \left( - \frac{1}{\theta} \sum_{i=1}^3 \frac{x_i}{i} \right).
    \end{align*}
    % For notational ease, let \(\varphi(\mathbf{x}) = \sum_{i=1}^3 x_i / i\), and so \(f(\mathbf{x} \,|\, \theta) = \mathrm{e}^{- \varphi(\mathbf{x}) / \theta} / 6 \theta^3\)
    and the corresponding log-likelihood function is given by 
    \(\ell(\mathbf{x} \,|\, \theta) = -3 \log (6 \theta) - \frac{1}{\theta} \sum_{i=1}^3 x_i / i.\)
    % and the corresponding log-likelihood function is given by 
    % \begin{align*}
    %     \ell(\mathbf{x} \,|\, \theta)
    %     = \log f(\mathbf{x} \,|\, \theta)
    %     = -3 \log (6 \theta) - \frac{1}{\theta} \sum_{i=1}^3 \frac{x_i}{i}.
    % \end{align*}
    Differentiating \(\ell(\mathbf{x} \,|\, \theta)\) with respect to \(\theta\), setting to 0, and solving for \(\theta\) gives us the MLE:
    \begin{align*}
        \frac{\partial \ell}{\partial \theta}  
        = - \frac{3}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^3 \frac{x_i}{i} 
        \overset{\text{set}}{=} 0
        ~~\Longrightarrow~~
        \hat{\theta}(\bm{X}) = \frac{1}{3} \sum_{i=1}^3 \frac{X_i}{i}
    \end{align*}
    \item[(b)] Let \(\psi = 1 / \theta\), and we believe that \(\psi \sim \mathrm{Gamma}(\alpha, \beta)\), i.e. 
    \(\xi(\psi) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \psi^{\alpha} \mathrm{e}^{-\beta \psi}\) for \(\psi > 0\). 
    For notational ease, let \(\varphi(\mathbf{x}) = \sum_{i=1}^3 x_i / i\); the likelihood function of \(\psi\) is then given by 
    \(f(\mathbf{x} \,|\, \psi) = \psi^3 \mathrm{e}^{- \varphi(\mathbf{x}) \psi} / 6\). We have 
    \begin{align*}
        \xi(\psi \,|\, \mathbf{x})
        \propto f(\mathbf{x} \,|\, \psi) \cdot \xi(\psi) 
        = \frac{1}{6} \psi^3 \mathrm{e}^{- \varphi(\mathbf{x}) \psi} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \psi^{\alpha - 1} \mathrm{e}^{-\beta \psi}
        \propto \psi^{\alpha + 3 - 1} \mathrm{e}^{-(\beta + \varphi(\mathbf{x})) \psi}.
    \end{align*}
    This is very similar to a Gamma distribution with parameters \(\tilde{\alpha} = \alpha + 3\) and \(\tilde{\beta} = \beta + \varphi(\mathbf{x})\), and adding 
    in the normalizing constants would make it so. Therefore, we conclude that \(\psi \,|\, \mathbf{x} \sim \mathrm{Gamma} \big( \alpha + 3, \beta + \varphi(\mathbf{x}) \big)\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 7} \noindent
% Let \(X \sim \mathrm{Unif}(0, \theta)\), so \(f(x \,|\, \theta) = 1 / \theta\) for \(0 \le x \le \theta\) (note then that \(\Theta = [x, \infty)\)). We assume 
% that \(\xi(\theta) = \theta \mathrm{e}^{-\theta}\). 
We assume that our parameter \(\theta\) has a prior density \(\xi(\theta) = \theta \mathrm{e}^{-\theta}\) for \(\theta > 0\). 
Let \(X \sim \mathrm{Unif}(0, \theta)\), and so \(f(x \,|\, \theta) = 1 / \theta\) for \(0 \le x \le \theta\). We note that while the original parameter space
was \(\Theta = (0, \infty)\), sampling from a uniform distribution who's bound depended on \(\theta\) resulted in a new parameter space 
\(\tilde{\Theta} = [x, \infty)\). 
The marginal distribution of \(X\) is given by 
\begin{align*}
    g_{X}(x) 
    = \int_{\tilde{\Theta}} f(x \,|\, \theta) \cdot \xi(\theta) \;\mathrm{d}\theta
    = \int_x^{\infty} \frac{1}{\theta} \cdot \theta \mathrm{e}^{- \theta} \;\mathrm{d}\theta
    = \int_x^{\infty} \mathrm{e}^{- \theta} \;\mathrm{d}\theta
    = \Big[ -\mathrm{e}^{-\theta} \Big]_x^{\infty}
    = \mathrm{e}^{-x},
\end{align*}
and so our posterior density for \(\theta\) is given by 
\begin{align*}
    \xi(\theta \,|\, x) 
    = \frac{f(x \,|\, \theta) \cdot \xi(\theta)}{g_X(x)}
    = \frac{1}{\theta} \cdot \theta \mathrm{e}^{-\theta} \cdot \frac{1}{\mathrm{e}^{-x}}
    = \mathrm{e}^{-(\theta - x)}.
\end{align*}
This denisty corresponds to a ``shifted'' exponential distribution with \(\lambda = 1\); instead of starting at zero, we are starting at \(x\).
\begin{itemize}
    \item[(a)] Using squared error loss, our Bayes' estimate is the mean of the posterior distribution:
    \begin{align*}
        \delta^*(x)
        = \mathbb{E}(\theta \,|\, x)
        = \int_x^{\infty} \theta \cdot \mathrm{e}^{-(\theta - x)} \;\mathrm{d}\theta
        = \Big[ -\theta \mathrm{e}^{-(\theta - x)} \Big]_x^{\infty} + \int_x^{\infty} \mathrm{e}^{-(\theta - x)} \;\mathrm{d}\theta
        % = \Big[ -\theta \mathrm{e}^{-(\theta - x)} \Big]_x^{\infty} + \int_x^{\infty} \mathrm{e}^{-(\theta - x)} \;\mathrm{d}\theta \\
        % &= \Big[ -\theta \mathrm{e}^{-(\theta - x)} \Big]_x^{\infty} + \Big[ - \mathrm{e}^{-(\theta - x)} \Big]_x^{\infty}
        % = x + \Big[ - \mathrm{e}^{-(\theta - x)} \Big]_x^{\infty}
        = x + 1.
    \end{align*}
    \item[(b)] Using absolute error loss, our Bayes' estimate is the median of the posterior distribution. 
    The posterior cdf is easily seen to be \(\Xi(\theta \,|\, x) = 1 - \mathrm{e}^{-(\theta - x)}\).
    To get the median, we need 
    \(\Xi \big( \delta^*(x) \big) = 1 - \mathrm{e}^{-(\delta^*(x) - x)} = 1/2\), and solving for \(\delta^*(x)\) gives us \(\delta^*(x) = x + \log 2\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 8} \noindent
Because \(0 \le \beta \le 1\), we have \(1/3 \le \theta \le 2/3\). If we are sampling from \(X \sim \mathrm{Ber}(\theta)\), then the likelihood 
and log-likelihood functions are given by 
\(f(\mathbf{x} \,|\, \theta) = \theta^{n \bar{x}}(1 - \theta)^{n(1 - \bar{x})}\) and 
\(\ell(\mathbf{x} \,|\, \theta) = n \bar{x} \log \theta + n(1 - \bar{x}) \log (1 - \theta).\)
% \begin{align*}
    % f(\mathbf{x} \,|\, \theta) &= \theta^{n \bar{x}}(1 - \theta)^{n(1 - \bar{x})} \\
    % \ell(\mathbf{x} \,|\, \theta) &= n \bar{x} \log \theta + n(1 - \bar{x}) \log (1 - \theta).
% \end{align*}  
Differentiating \(\ell(\mathbf{x} \,|\, \theta)\), setting equal to zero, and solving for \(\theta\) gives the MLE as
\begin{align*}
    \frac{\partial \ell(\mathbf{x} \,|\, \theta)}{\partial \theta}
    = \frac{n \bar x}{\theta} - \frac{n(1 - \bar{x})}{1 - \theta}
    % = n \left( \frac{\bar x}{\theta} - \frac{1 - \bar{x}}{1 - \theta} \right)
    = n \left( \frac{\bar{x} - \theta}{\theta(1 - \theta)} \right)
    \overset{\text{set}}{=} 0
    ~~\Longrightarrow~~ \hat{\theta} = \bar{x}.
\end{align*}
% need to re-write this better
We must be cautious; because each \(x_i \in \{0,1\}\), we can have \(\bar{x} \in [0,1]\), and so the maximum of \(\ell(\mathbf{x} \,|\, \theta)\)
can occur at \(\hat{\theta} \in [0,1]\). However, because of the contraints placed on \(\theta\) by \(\beta\), this maximum can potentially fall outside the range of 
possible values. To remedy this, we will consider two cases:
\begin{enumerate}
    % \item \(\bar{x} < 1/3\): we have \(\partial \ell / \partial \theta \propto 1/3 - \theta\), so for all values 
    \item \(\bar{x} < 1/3\): for all values \(\theta \in [1/3, 2/3]\), we have \(\partial \ell / \partial \theta < 0 \), so \(\ell(\mathbf{x} \,|\, \theta)\)
    is a decreasing function. Then the maximum value of \(\ell(\mathbf{x} \,|\, \theta)\) is obtained when \(\theta = 1/3\).
    \item \(\bar{x} > 2/3\): for all values \(\theta \in [1/3, 2/3]\), we have \(\partial \ell / \partial \theta > 0 \), so \(\ell(\mathbf{x} \,|\, \theta)\)
    is an increasing function. Then the maximum value of \(\ell(\mathbf{x} \,|\, \theta)\) is obtained when \(\theta = 2/3\).
\end{enumerate}
Therefore, the MLEs for both \(\theta\) and \(\beta\) are given by 
\begin{align*}
    \hat{\theta} 
    = \begin{cases}
        \hfil \bar{X} & \text{if \(1/3 \le \bar{X} \le 2/3\)}, \\
        1/3 & \text{if \(\bar{X} < 1/3\)}, \\
        2/3 & \text{if \(\bar{X} > 2/3\)}.
    \end{cases}
    ~~~\text{and}~~~
    \hat{\beta}
    = 3 \hat{\theta} - 1
    = \begin{cases}
        3\bar{X} - 1 & \text{if \(1/3 \le \bar{X} \le 2/3\)}, \\
        \hfil 0 & \text{if \(\bar{X} < 1/3\)}, \\
        \hfil 1 & \text{if \(\bar{X} > 2/3\)}.
    \end{cases}
\end{align*}
This is because \(\beta = 3 \theta - 1\), which means the MLE of \(\beta\) is given by \(\hat{\beta} = 3 \hat{\theta} - 1\).
% \begin{align*}
%     \hat{\beta}
%     = 3 \hat{\theta} - 1
%     = \begin{cases}
%         3\bar{X} - 1 & \text{if \(1/3 \le \bar{X} \le 2/3\)}, \\
%         \hfil 0 & \text{if \(\bar{X} < 1/3\)}, \\
%         \hfil 1 & \text{if \(\bar{X} > 2/3\)}.
%     \end{cases}
% \end{align*}

%' ============================================================================================================================================================
\section{Question 9} \noindent
We are sampling from a ``shifted'' exponential distribution, i.e. its density is given by \(f(x \,|\, \beta, \theta) = \beta \mathrm{e}^{-\beta (x - \theta)}\)
for \(x \ge \theta\). The likelihood function is then given by \(f(\mathbf{x} \,|\, \beta, \theta) = \beta^n \mathrm{e}^{-n \beta (\bar{x} - \theta)}\) when
\(x_i \ge \theta \) for \(i \in \{1, \ldots, n\}\). If every observation \(x_i \ge \theta\), then it is also true that the lowest value for each observation
is as well, so \(x_{(1)} \ge \theta\). We can incorporate this condition into the likelihood function using an indicator function:
\begin{align*}
    f(\mathbf{x} \,|\, \beta, \theta) 
    % = \beta^n \mathrm{exp} \left( - n \beta (\bar{x} - \theta) \right)
    = \beta^n \mathrm{e}^{-n \beta (\bar{x} - \theta)} \cdot \mathbb{I}_{[\theta, \infty)}(x_{(1)}).
    % = 1 \cdot \beta^n \mathrm{e}^{-n \beta (\bar{x} - \theta)} \cdot \mathbb{I}_{[\theta, \infty)}(x_{(1)})
    % = \Big( \beta^n \Big) \cdot \Big( \mathrm{e}^{-n \beta (\bar{x} - \theta)} \cdot \mathbb{I}_{[\theta, \infty)}(x_{(1)}) \Big)
\end{align*}
By the Factorization Theorem, \(\bar{X}\) and \(X_{(1)}\) are a pair of jointly sufficient statistics for \(\beta\) and \(\theta\). 

%' ============================================================================================================================================================
\section{Question 10} \noindent
% Let \(\mathbf{X} = (X_1, \ldots, X_n)^T\) be a random sample from a Pareto distribution, so 
% \(f(x_i \,|\, \alpha, x_0) = \alpha x_0^{\alpha} / x_i^{\alpha + 1} \mathbb{I}_{[x_0, \infty)}(x_i)\). The likelihood function is given by 
% \begin{align*}
%     f(\mathbf{x} \,|\, \alpha, x_0)
%     % = \prod_{i=1}^n \alpha x_0^{\alpha} / x_i^{\alpha + 1} \mathbb{I}_{[x_0, \infty)}(x_i)
%     = \prod_{i=1}^n \frac{\alpha x_0^{\alpha}}{x_i^{\alpha + 1}} \mathbb{I}_{[x_0, \infty)}(x_i)
%     = \alpha^n x_0^{n \alpha} \left( \prod_{i=1}^n \frac{1}{x_i} \right)^{\alpha + 1} \mathbb{I}_{[x_0, \infty)}(x_{(1)})
% \end{align*}
Let \(x_1, \ldots, X_n \overset{\text{iid}}{\sim} \mathrm{Pareto}(\alpha, x_0)\), where \(\alpha > 0\) is known and \(x_0 > 0\) is unknown. The likelihood 
function is given by 
\begin{align*}
    f(\mathbf{x} \,|\, \alpha, x_0) 
    = \prod_{i=1}^n \frac{\alpha x_0^{\alpha}}{x_i^{\alpha + 1}}
    = \alpha^n x_0^{\alpha n} \left( \prod_{i=1}^n \frac{1}{x_i} \right)^{\alpha + 1}
    % = \frac{\alpha^n}{\prod_{i=1}^n x_i^{\alpha + 1}} x_0^{\alpha n}
    = C(\mathbf{x}, \alpha) \cdot x_0^{\alpha n},
    ~~~\text{where}~~~
    C(\mathbf{x}, \alpha) = \alpha^n \left( \prod_{i=1}^n \frac{1}{x_i} \right)^{\alpha + 1},
\end{align*}
when \(x_i \ge x_0\) for \(i \in \{1, \ldots, n\}\). 
This is the same as saying \(x_{(1)} \ge x_0\), where \(x_{(1)}\) is the first order statistic, and so 
our new parameter space for \(x_0\) is \((0, x_{(1)}]\) (it was originally \((0, \infty)\)).
In the likelihood function, \(C(\mathbf{x}, \alpha)\) is a constant (with respect to \(x_0\)) that depends on \(\mathbf{x}\) and \(\alpha\). 
We can see that for \(x_0 \in (0, x_{(1)}]\), \(f(\mathbf{x} \,|\, \alpha, x_0)\) is an increasing function, so its maximum will be obtained 
at the largest possible value, \(x_{(1)}\). Therefore, our MLE is \(\hat{x}_0 = X_{(1)}\). 

%' ============================================================================================================================================================
\section{Question 11} \noindent
From the previous question, by incorporating indicator functions, our likelihood function can be written as 
\begin{align*}
    f(\mathbf{x} \,|\, \alpha, x_0)
    = C(\mathbf{x}, \alpha) \cdot x_0^{\alpha n} \cdot \mathbb{I}_{[x_0, \infty)}(x_{(1)}).
\end{align*}
By the Factorization Theorem, where \(u(\mathbf{x}) = C(\mathbf{x}, \alpha)\) and \(v(x_{(1)}, x_0) = x_0^{\alpha n} \cdot \mathbb{I}_{[x_0, \infty)}(x_{(1)})\),
the first order statistic \(X_{(1)}\) is a sufficient statistic for \(x_0\). Since it is also the MLE of \(x_0\), is follows that \(X_{(1)}\) is a minimal
sufficient statistic.



\end{document}