\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



% \titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Homework 2}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5204: Statistical Inference}\\
    {\normalsize Columbia University}
}
\date{\normalsize Novermber 26, 2020}

\begin{document}

\maketitle

%' ============================================================================================================================================================
\section{Question 1} \noindent
Suppose that \(Y \sim \mathrm{Bin}(100,p)\), and we want to make an inference about the value of \(p\). We test \(H_0 : p = 0.08\) against \(H_A : p < 0.08\),
and our test \(\delta\) will reject \(H_0\) if and only if \(Y = 6\).
\begin{itemize}
    \item[(a)] The significance \(\alpha\) is the probability of making a Type I error, 
    \begin{align*}
        \alpha(\delta)
        = \mathrm{Pr}(Y = 6 \,|\, p = 0.08)
        = \binom{100}{6} (0.08)^6 (0.92)^{94}
        = 0.1232795.
    \end{align*}
    \item[(b)] Suppose that \(p = 0.04\). The probability of a Type II error, \(\beta\), is
    \begin{align*}
        \beta(\delta)
        = \mathrm{Pr}(Y \neq 6 \,|\, p = 0.04)
        = 1 - \mathrm{Pr}(Y = 6 \,|\, p = 0.04)
        = 1 - \binom{100}{6} (0.04)^6 (0.96)^{94}
        = 0.8947672.
    \end{align*}
\end{itemize}

\newcommand{\zgam}{z_{\gamma}}
%' ============================================================================================================================================================
\section{Question 2} \noindent
For a random variable \(Y \sim \mathrm{Binom}(n, p)\), if \(n\) is large enough we can approximate it using a normal distribution with the same mean and 
variance, i.e. \(Y \sim \mathrm{N}(np, np(1-p))\). Then the sample proportion, \(\hat{p} = Y / n\), is also normally distributed as 
\(\hat{p} \sim \mathrm{N}(p, p(1-p)/n)\), and standardizing \(\hat{p}\) gives us \((\hat{p} - p) / \sqrt{p(1-p)/n} \sim \mathrm{N}(0,1)\). 
% maybe put derivation in appendix?
It can then be shown that a \(100\gamma\)\%{} confidence interval for \(\hat{p}\) is given by 
\begin{align*}
    \mathcal{I}
    = \left( ~ 
    \frac{\hat{p} + \zgam^2 / 2n}{1 + \zgam^2 / 2n} - \zgam \cdot \frac{\sqrt{\hat{p}(1 - \hat{p}) / n + \zgam^2 / 4n^2}}{1 + \zgam^2 / n} 
    ~,~ 
    \frac{\hat{p} + \zgam^2 / 2n}{1 + \zgam^2 / 2n} + \zgam \cdot \frac{\sqrt{\hat{p}(1 - \hat{p}) / n + \zgam^2 / 4n^2}}{1 + \zgam^2 / n}
    ~ \right),
\end{align*}
where \(\zgam = \Phi^{-1}((1 + \gamma)/2)\). For this example, we have \(n = 300\) and \(\hat{p} = 75 / 300 = 1/4\), and to get a \(90\)\%{}
confidence interval, we have \(z_{0.90} = \). Therefore, a \(90\)\%{} confidence interval for \(p\) is 

%' ============================================================================================================================================================
\section{Question 9} \noindent
If \(\bm{X} \overset{\mathrm{iid}}{\sim} \mathrm{N}(\mu, \sigma^2)\) with unknown \(\mu\) and \(\sigma^2\), then a \(\gamma\)\%{} confidence interval
for \(\mu\) is given by 
\begin{align*}
    \mathcal{I} = \Big( ~\bar{X} - t_{\gamma}(n) \cdot S / \sqrt{n} ~,~ \bar{X} + t_{\gamma}(n) \cdot S / \sqrt{n} ~ \Big),
\end{align*}
where \(t_{\gamma}(n) = T_{n-1}^{-1} ( (1 + \gamma) / 2 )\) is the \((1 + \gamma) / 2\)th quantile of the \(t\) distribution with \(\mathrm{df} = n-1\) and \(S\)
is the sample standard deviation. 
The length of this confidence interval is given by 
\begin{align*}
    \Delta 
    % = \bar{X} + T_{n-1}^{-1}\big( (1 + \gamma) / 2 \big) \frac{S}{\sqrt{n}} - \bigg( \bar{X} - T_{n-1}^{-1}\big( (1 + \gamma) / 2 \big) \frac{S}{\sqrt{n}} \bigg)
    = \max (\mathcal{I}) - \min (\mathcal{I})
    = \Big( \bar{X} + t_{\gamma}(n) \cdot S / \sqrt{n} \Big) - \Big( \bar{X} - t_{\gamma}(n) \cdot S / \sqrt{n} \Big)
    = 2 t_{\gamma}(n) \cdot S / \sqrt{n}.
\end{align*}
The squared length is then given by \(\Delta^2 = 4 t_{\gamma}^2(n) \cdot S^2 / n\). 
Because the sample variance is an unbiased estimator for \(\sigma^2\), we have \(\mathbb{E}[\Delta^2] = \mathbb{E} \big[ 4 t_{\gamma}^2(n) \cdot S^2 / n \big] = 4 t_{\gamma}^2(n) \cdot \sigma^2 / n\).
We now set \(\mathbb{E}[\Delta^2] < \sigma^2 / 2\), and after some cancellations, we see that we need \(t_{\gamma}^2(n) / n < 1/8\). There is no way to find a
closed-form expression for this, so we will have to check the value of \(t_{\gamma}^2(n) / n\) for increasing values of \(n\). I set up a \texttt{while} 
loop in \texttt{R} to solve for it, and when \(\gamma = 0.9\), we find that \(n = 24\) is the smallest value of \(n\) such that \(\mathbb{E}[\Delta^2] < \sigma^2 / 2\).

%' ============================================================================================================================================================
\section{Question 10} \noindent
Let \(\bm{X} \overset{\mathrm{iid}}{\sim} \mathrm{N}(\theta, \sigma^2)\), where \(\theta\) is unknown and \(\sigma^2\) is known, and we assume prior that 
\(\theta \sim \mathrm{N}(\mu, \nu^2)\), where both \(\mu\) and \(\nu^2\) are known. 
\begin{itemize}
    \item[(a)] Since normal distributions are are conjugate to normal sampling, it follows
    that \(\theta \,|\, \mathbf{x} \sim \mathrm{N}(\tilde{\mu}, \tilde{\sigma}^2)\), where 
    \begin{align*}
        \tilde{\mu}
        = \frac{\sigma^2 \mu + n \nu^2 \bar{x}}{\sigma^2 n \nu^2}
        ~~~\text{and}~~~
        \tilde{\sigma}^2
        = \frac{\sigma^2 \nu^2}{\sigma^2 + n \mu^2}.
    \end{align*}
    We also know that \((\theta \,|\, \mathbf{x} - \tilde{\mu}) / \tilde{\sigma} \sim \mathrm{N}(0,1)\), and so a \(95\)\%{} confidence interval for 
    \(\theta \,|\, \mathbf{x}\) is given by 
    \begin{align*}
        \mathcal{I}
        = \Big( ~ \tilde{\mu} - \Phi^{-1}(0.975) \cdot \tilde{\sigma} ~,~ \tilde{\mu} + \Phi^{-1}(0.975) \cdot \tilde{\sigma} ~ \Big).
        % = \Big( ~ \frac{\sigma^2 \mu + n \nu^2 \bar{x}}{\sigma^2 n \nu^2} - \Phi^{-1}(0.975) \cdot \frac{\sigma^2 \nu^2}{\sigma^2 + n \mu^2} ~,~ \frac{\sigma^2 \mu + n \nu^2 \bar{x}}{\sigma^2 n \nu^2} + \Phi^{-1}(0.975) \cdot \frac{\sigma^2 \nu^2}{\sigma^2 + n \mu^2} ~ \Big)
    \end{align*} 
    \item[(b)] We can think of our interval \(\mathcal{I}\) as a function of \(\nu^2\). To examine what happens to \(\mathcal{I}(\nu^2)\) as \(\nu^2 \to \infty\), we 
    will first look at \(\tilde{\mu}\) and \(\tilde{\sigma}\). Using L'Hopital's rule, we have
    \begin{align*}
        \lim_{\nu^2 \to \infty} \tilde{\mu}
        &= \lim_{\nu^2 \to \infty} \frac{\sigma^2 \mu + n \nu^2 \bar{x}}{\sigma^2 n \nu^2}
        = \lim_{\nu^2 \to \infty} \frac{n \bar{x}}{n}
        = \bar{x}, \\
        \lim_{\nu^2 \to \infty} \tilde{\sigma}
        &= \lim_{\nu^2 \to \infty} \sqrt{\frac{\sigma^2 \nu^2}{\sigma^2 + n \mu^2}}
        = \sqrt{ \lim_{\nu^2 \to \infty} \frac{\sigma^2 \nu^2}{\sigma^2 + n \mu^2}}
        = \sqrt{ \lim_{\nu^2 \to \infty} \frac{\sigma^2}{n}}
        % = \sqrt{\frac{\sigma^2}{n}}
        = \frac{\sigma}{\sqrt{n}},
    \end{align*}
    and so \(\mathcal{I}(\nu^2) \to \big( ~\bar{x} - \Phi^{-1}(0.975) \cdot \sigma / \sqrt{n} ~,~ \bar{x} + \Phi^{-1}(0.975) \cdot \sigma / \sqrt{n} ~ \big)\),
    which is a \(95\)\%{} confidence interval for \(\theta\). 
\end{itemize}

%' ============================================================================================================================================================
\section{Question 11} \noindent
Let \(\bm{X} \overset{\mathrm{iid}}{\sim} \mathrm{Unif}(0, \theta)\), where \(\theta\) is unknown, and let \(Y = X_{(n)}\) be the \(n\)th order statistic. 
\begin{itemize}
    \item[(a)] Let \(F_i(x) = \mathrm{Pr}(X_i \le x) = x / \theta\) for all \(i \in \{1, \ldots, n\}\). Then the cdf of \(Y\) is 
    \(G(y) = \prod_{i = 1}^n F_i(y) = \left( y / \theta \right)^n\), since all of the \(X_i\)'s are independent. The density of \(Y\) is then given by 
    \(g(y) = n y^{n - 1} / \theta^n\) for \(y \in [0, \theta]\). By letting \(W = Y / \theta\), we have \(Y = \theta W\) and \(\partial Y / \partial W = \theta\), 
    so the density of \(Y / \theta\) is given by \(h(w) = g\big( y(w) \big) \cdot \theta = n w^{n-1}\) for \(w \in [0,1]\). The cdf is then seen to be 
    \(H(w) = w^n\), and so the quantile is given by \(H^{-1}(w) = \sqrt[n]{w}\).
    % \begin{align*}
    %     G(y) 
    %     = \mathrm{Pr}(Y \le y)
    %     = \mathrm{Pr} \big( X_1 \le y \cap \cdots \cap X_n \le y \big)
    %     = \prod_{i = 1}^n F_i(y) = \left( \frac{y}{\theta} \right)^n
    % \end{align*}
    \item[(b)] Since we must have \(y \le \theta\), it is natural that \(Y\) will underestimate \(\theta\), and is therefore biased. We have 
    \begin{align*}
        \mathbb{E}[Y]
        = \int_{0}^{\theta} y \cdot \frac{n y^{n-1}}{\theta^n} \,\partial y
        = \frac{n}{\theta^n} \int_0^{\theta} y^n \,\partial y
        = \frac{n}{\theta^n} \cdot \frac{y^{n+1}}{n+1} \bigg|_{0}^{\theta}
        = \frac{n}{\theta^n} \cdot \frac{\theta^{n+1}}{n+1}
        % = \frac{n}{n+1} \cdot \theta,
        = \frac{n\theta}{n+1},
    \end{align*}
    and so the bias is \(\mathrm{bias}(Y) = \mathbb{E}[Y] - \theta = -\theta / (n+1)\). 
    \item[(d)] Using the cdf of \(Y / \theta\), for any interval involving \(Y / \theta\), we have \(\mathrm{Pr}(a \le Y / \theta \le b) = b^n - a^n\), where 
    \(a, b \in (0, 1]\). Rearranging the terms inside the interval gives us \(\mathrm{Pr}(Y / b \le \theta \le Y / a) = b^2 - a^2 \overset{\mathrm{set}}{=} \gamma\).
    That is, as long as we impose the constraint that \(b^2 - a^2 = \gamma\), any interval \((Y/b, Y/a)\) is a \(\gamma\)\%{} confidence interval for \(\theta\). 
\end{itemize}

%' ============================================================================================================================================================
\section{Question 12} \noindent
Suppose that \(X \sim \mathrm{P}\), where \(\mathrm{P}\) is an unknown distribution, and we want to test \(H_0: \mathrm{P} = \mathrm{Unif}(0,1)\) against 
\(H_1: \mathrm{P} = \mathrm{N}(0, 1)\). The densities of both distributions (within their support) are given by \(f_0(x) = 1\) for \(x \in [0,1]\) and 
\(f_2(x) = (2\pi)^{-1/2}\mathrm{exp}(-x^2 / 2)\), respectively, and so our test statistics is 

%Determine the most powerful test of size 0.01, and calculate the power of the test when H1 is true.

\end{document}